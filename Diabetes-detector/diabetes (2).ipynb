{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Step 1: Load the dataset (adjust path if needed)\n",
        "data = pd.read_csv('/content/diabetes.csv')\n",
        "\n",
        "# Step 2: Check the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Preprocessing\n",
        "# Check for missing values and handle them if necessary\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# If there are any missing values, fill them with mean or median (as needed)\n",
        "# Example:\n",
        "# data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "# Step 4: Feature selection (assuming the last column is the target variable)\n",
        "X = data.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "y = data.iloc[:, -1]   # Target (last column, which is the 'Outcome')\n",
        "\n",
        "# Step 5: Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Build the SVM Model\n",
        "model = SVC(kernel='linear')  # You can change the kernel if needed, e.g., 'rbf', 'poly', etc.\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the Model\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 10: Visualization (Optional but useful)\n",
        "# Confusion Matrix Visualization\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=['No Diabetes', 'Diabetes'], yticklabels=['No Diabetes', 'Diabetes'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "LmewkGv1EdFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing additional libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Step 1: Load the dataset (adjust path if needed)\n",
        "data = pd.read_csv('/content/diabetes.csv')\n",
        "\n",
        "# Step 2: Check the first few rows of the dataset\n",
        "print(data.head())\n",
        "\n",
        "# Step 3: Preprocessing\n",
        "# Check for missing values and handle them if necessary\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# If there are any missing values, fill them with mean or median (as needed)\n",
        "# Example:\n",
        "# data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "# Step 4: Feature selection (assuming the last column is the target variable)\n",
        "X = data.iloc[:, :-1]  # Features (all columns except the last one)\n",
        "y = data.iloc[:, -1]   # Target (last column, which is the 'Outcome')\n",
        "\n",
        "# Step 5: Train-Test Split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Step 7: Build the SVM Model\n",
        "from sklearn.svm import SVC\n",
        "model = SVC(kernel='linear')  # You can change the kernel if needed, e.g., 'rbf', 'poly', etc.\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the Model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Step 10: Additional Visualizations\n",
        "\n",
        "## 1. Distribution of the target variable ('Outcome')\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='Outcome', data=data, palette='coolwarm')\n",
        "plt.title('Distribution of Diabetes (Outcome)')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "## 2. Feature distribution (for each feature)\n",
        "features = X.columns\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, feature in enumerate(features):\n",
        "    plt.subplot(3, 3, i + 1)\n",
        "    sns.histplot(data[feature], kde=True, color='blue')\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "## 3. Correlation heatmap between features\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n",
        "\n",
        "## 4. If there are two significant features, visualize the SVM decision boundary (using PCA for 2D representation)\n",
        "if X.shape[1] == 2:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=X_test[:, 0], y=X_test[:, 1], hue=y_test, palette='coolwarm', marker='o')\n",
        "\n",
        "    # Decision Boundary\n",
        "    xx, yy = np.meshgrid(np.linspace(X_test[:, 0].min(), X_test[:, 0].max(), 100),\n",
        "                         np.linspace(X_test[:, 1].min(), X_test[:, 1].max(), 100))\n",
        "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.title('SVM Decision Boundary')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.show()\n",
        "\n",
        "## 5. If the data has more than 2 features, use PCA to reduce the dimensions to 2 for decision boundary visualization\n",
        "else:\n",
        "    pca = PCA(n_components=2)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    X_train_pca = pca.transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=X_test_pca[:, 0], y=X_test_pca[:, 1], hue=y_test, palette='coolwarm', marker='o')\n",
        "\n",
        "    # SVM decision boundary in 2D PCA space\n",
        "    xx, yy = np.meshgrid(np.linspace(X_test_pca[:, 0].min(), X_test_pca[:, 0].max(), 100),\n",
        "                         np.linspace(X_test_pca[:, 1].min(), X_test_pca[:, 1].max(), 100))\n",
        "    Z = model.predict(pca.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.title('SVM Decision Boundary (PCA-reduced)')\n",
        "    plt.xlabel('PCA 1')\n",
        "    plt.ylabel('PCA 2')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mMXpU1hzExbc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}